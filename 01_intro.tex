\chapter{Introduction}\label{ch:intro}

In the midst of the fast-paced advancements in data processing and even faster-paced data acquisition, we currently are living in the world and age of increasing data abundance. While significant improvements in computing power enabled feasible information extraction processes, created the data-centric business practice and opened new possibilities in research in the first place, the concurrent rising acceptance of data collecting technologies and the hunger for more data in hopes for later use, rapidly increased and overtook the processible amount of data. 
%  driven by e.g. increasingly powerful smart devices and other user behaviour tracking practices, increased the amount of data sources rapidly,
%Therefore even with an abundance of data the actual problem of information scarcity still applies.

This raw data often is present in the form of vast amounts of observations consisting of several features represented as numerical values and thus is hardly comprehensible for human insight or intuition. Without a given procedure to extract meaningful information out of the data, this data abundance would be mostly useless for immediate human processing. One out-dated way to cope with this issue is the manual labelling of each observation by human hand, which however obviously is getting more and more unrealistic since with the increasing amounts of data this task is getting inefficient and/or even infeasible. Therefore an automated method to extract relevant information is becoming essential to analyzing the exploding amounts of data. %To tackle this problem, there has been done much research in the fields of Deep- and Unsupervised Learning.

To deal with this problem, there has been lots of different approaches.
If the actual relevant features are not required and just the means to achieve a goal, the analyzing task can be solved via implicit feature engineering, where it is assumed that the analyzing model learns relevant features by itself.
However if not, the analyzing task can be approached via unsupervised knowledge discovery processes, with goals such as a segmentation of data sets by some shared attributes, a simplification of data sets by aggregating variables with similarity or the detection of anomalies or correlations.

In tasks working with high-dimensional data sets, such as genome analysis in biology/medicine and text analysis in \gls{nlp}, it is particularly interesting, which \textit{features} play important roles in characterizing clusters. This task is called Subspace Clustering. In the special case of an additional interest in how the important features relate to each other, the task of \textit{Correlation Clustering} arises.

The problem with existing Subspace and Correlation Clustering approaches is, that they are restricted to a given scope, either local or global, and do not have the capabilities to find the opposite scopes result. This is inconvenient and limits the thorough evaluation of clusterings if both scopes are of interest. Our goal is to combine existing clustering paradigms to create a correlation clustering algorithms for both scopes simultaneously.

% does not restrict itself to the economic field but is apparent in various fields of science, such as


% This work in particular aims to tackle the
% In this work, we focus on an unsupervised solution for this problem. Namely, we propose a novel algorithm, which combines two different concepts to get a universal clustering of the data.

% \acrfull{cash} is cool, however its complexity is $\mathcal{O}(n^2)$.
% We try to do it better by using a density-based approach to prune away irrelevant points and noise.

\section{Problem Statement}
Our proposal is a correlation clustering algorithm, which builds upon an existing local classical clustering approach and a global correlation clustering approach. It combines those into a more exhaustive algorithm which can give a more comprehensive insight, notably a simultaneously local and global view, onto the applied data and while additionally providing a similar performance compared to the global correlation clustering approach. In particular, we first apply a density-based approach onto the data sets to filter out relevant subintervals of the original data space (not to be confused with subspaces) and evaluate those by applying a correlation clustering algorithm based on the Hough Transform. Those local results are then stitched together and used to reevaluate previously (falsely) pruned points, to get a complete evaluation of every point of the data set. With this approach, we receive a labelling of a local view and also a global view of the points. 

\section{Structure of the Thesis}
The structure of this thesis is organized as follows:

First off, \textit{\autoref{ch:intro}} introduces/illustrates the motivation of our thesis using tasks/problems from various fields of the current age.
\textit{\hyperref[ch:overview]{Chapter~\ref*{ch:overview}}} gives a general overview of the terminology of clustering, provides a general structure its different archetypes and leads the reader towards our unique use case.
\textit{\hyperref[sec:Related Work]{Chapter~\ref*{sec:Related Work}}} covers a collection of related works not able to cluster in the desired manner with regards to Correlation Clustering and introduces components necessary to our algorithm to tackle these problems.
In \textit{\autoref{ch:Methods}}, we provide an in-depth explanation of our algorithms core components and give an extensive view of the separate steps of our novel approach.
\textit{\hyperref[ch:evaluation]{Chapter~\ref*{ch:evaluation}}} examines our algorithms' runtime, and clustering performance with regards to different data set characteristics, such as numbers of data objects, amount of noise and dimensionality, and compares its result to its ancestor/peer \acrshort{cash}.
To wrap up this work \textit{\autoref{ch:conclusion}} concludes this thesis by summarizing the current state of research in the field of Correlation Clustering, and based on the results, aims to categorize our approach within this field. As an outlook \textit{\autoref{ch:futurework}} points out possible future work.

% First off \textit{\autoref{ch:intro}} introduces/illustrates the motivation of our thesis using tasks/problems from various fields, e.g. biology, medicine, economy, etc., of the current age.
% \textit{\hyperref[ch:overview]{Chapter~\ref*{ch:overview}}} gives a general overview of the terminology of clustering, provides a general structure of the different archetypes and leads the reader towards our special type of clustering where our use case is relevant.
% \textit{\hyperref[sec:Related Work]{Chapter~\ref*{sec:Related Work}}} covers a collection of related works not able to cluster in the desired manner with regards to Correlation Clustering and introduces components necessary to our algorithm to tackle these problems.
% In \textit{\autoref{ch:Methods}} we provide an in depth explanation of our algorithms core components and give an extensive view to the separate steps of our novel approach.
% \textit{\hyperref[ch:evaluation]{Chapter~\ref*{ch:evaluation}}} examines our algorithms runtime and clustering performance with regards to different data set characteristics, such as numbers of data objects, amount of noise and dimensionality, and compares its result to its ancestor/peer \acrshort{cash}.
% To wrap up this work \textit{\autoref{ch:conclusion}} concludes this thesis by summarizing the current state of research in the field of Correlation Clustering, and based on the results, aims to categorize our approach within this field. As an outlook \textit{\autoref{ch:futurework}} points out possible future work.

% Note that in this thesis \textit{subspaces} and \textit{subspace clusters}, except in \autoref{sec:clu} or unless explicitly stated, always refer to \textit{linear} arbitrarily oriented subspaces/correlations and its clusters. We also use \textit{Subspace Clustering} and \textit{Correlation Clustering} interchangeably.
% \todor{Notiz behalten?}