\chapter{Introduction}

In the midst of fast-paced advancements in data gathering we currently are living in the world and age of data abundance. This data however often is present in the form of huge amounts of observations consisting of several features represented as numerical values and thus does not provide much human insight or intuition. Therefore this abundance does not solve our actual problem of information scarcity. Without a given procedure to extract important information out of the data this data abundance would therefore be mostly useless for immediate human processing. One old way to cope with this issue is the manual labeling of each observation by human hand, which however obviously is getting more and more unpopular since with the amount of data this task is already getting inefficient or even infeasible. %To tackle this problem there has been done a lot of research in the fields of Deep- and Unsupervised Learning. 
To tackle this problem there has been lots of different approaches. Either via implicit feature engineering, where we are assuming that the model learns relevant features by itself, or automatic knowledge discovery processes in the field of unsupervised learning, where the goal is  the segmentation of datasets by some shared attributes or the simplification of datasets by aggregating variables with similiarity or detecting anomalies or correlations. Each approach has their own strength and weaknesses

In this work we focus on the unsupervised solution for this problem. Namely we try to create a new algorithm, which combines some approaches to get a more universal clustering of the data.

Clustering in Arbitrary Subspaces based on the Hough transform (CASH) is cool, however its complexity is $\mathcal{O}(n^2)$.
We try to do it better via using a density based approach to prune away irrelevant points and noise..

\section{Problem Statement}
Our proposol is an algorithm, which builds upon existing local and global clustering approaches and combines those into a more exhaustive algorithm which can give even more insights onto the applied data. In particular we firstly are apply a density based approach onto the data sets to filter out relevant subintervals of the original space (not to be confused with subspaces) and evaluate those by applying a correlation clustering algorithm. Those local results are then stitched together and used to reevaluate every other previously pruned point, to get a complete evaluation of every point of the dataset. With this approach, we receive a labeling of a local view and also a global view of the points. 

\section{Structure of the Thesis}
