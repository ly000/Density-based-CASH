\chapter{Introduction}

In the midst of fast-paced advancements in data gathering we currently are living in the world and age of data abundance. This data however often is present in the form of huge masses of observations consisting of another bunch of features represented as numerical values and thus does not provide much human insight or intuition. Therefore this abundance does not solve our actual problem of information scarcity. Without a given procedure to extract important information out of the data this data abundance would therefore be mostly useless for immediate human processing. One old way to cope with this issue is the manual labeling of each observation by human hand, which however obviously is getting more and more unpopular since with the amount of data this task is already getting inefficient or even infeasible. %To tackle this problem there has been done a lot of research in the fields of Deep- and Unsupervised Learning. 
To tackle this problem there has been lots of different approaches. Either via implicit feature engineering, where we are assuming that the model learns relevant features by itself, or automatic knowledge discovery processes in the field of unsupervised learning, where the goal is to segment datasets by some shared attributes/simplify datasets by aggregating variables with similiarity or detecting anomalies or correlations

In this work we focus on the unsupervised solution for this problem. Namely we 

Clustering in Arbitrary Subspaces based on the Hough transform (CASH) is cool, however its complexity is $\mathcal{O}(n^2)$.
We try to do it better via using a density based approach to prune away irrelevant points and noise..

