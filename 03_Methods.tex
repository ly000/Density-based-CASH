\chapter{Methods}\label{ch:Methods}
In this chapter we introduce the necessary mathematical and algorithm foundations, give an indepth view of its components and explain their relevance in the new approach.

In this chapter 
Clustering in Arbitrary Subspaces based on the Hough transform (CASH) is cool, however its complexity is $\mathcal{O}(n^2)$.
We try to do it better via using a density based approach to prune away irrelevant points and noise..

\section{Mathematical and Algorithmic Foundations [4]}
\label{sec:Foundations}
Our Algorithm builds on top of two algorithms which both were build on two other well-known algorithms: OPTICS, which is closely related to DBSCAN, and CASH, which is inherently based on the Hough Transform.
\subsection{DBSCAN}
\label{ssec:DBSCANindepth}
\begin{figure}
    \centering
    \begin{minipage}{.47\textwidth}
      \centering
      \includegraphics[width=.8\textwidth]{figures/DSwithDBSCANBoundingBoxes.pdf}
      \captionof{figure}{A figure}
      \label{fig:test1}
    \end{minipage}%
    \begin{minipage}{.53 \textwidth}
      \centering
      \includegraphics[width=.8\textwidth]{figures/DBSCANwithNoise.pdf}
      \captionof{figure}{Another figure}
      \label{fig:test2}
    \end{minipage}
\end{figure}
To begin with we voted for the use of DBSCAN to retrieve the dense intervals of the original dataset. 
As mentioned before the algorithm DBSCAN is categorized into the density based approach of clustering. This means, that it considers points connected through low proximity to each other into similar clusters and thus can, in contrast to partitioning based approaches, create clusters of arbitrary shape (cf. \autoref{fig:kmeansdbscan}). Hence these clusters also can retain information more accurate information about the correlation of the data. Additionally it also directly eliminates noise and can therefore lower the computational cost of the future steps. Its basic principle is as follows:
A group of points is considered as a dense cluster when for each point of the cluster there are enough other points in its neighborhood~\cite{DBSCANEKSX96}.

To understand, how this algorithm works we first have to introduce some terminology.
Given a data set $DS$ of data objects $o$ with a dimensionality of $N$, a distance measure $dist(p_1,p_2))$ and two new parameters, the Epsilon distance $\epsilon$ and the minimum number of points to be a cluster $MinPts$, \todo{cite style} \citeauthor{DBSCANEKSX96} defines:

\subsubsection*{$\epsilon$-Neighborhood}
% \begin{defn}
The $\epsilon$-Neighborhood $N_{\epsilon}(p)$ of a point $p$ is defined by the set of points $q$ which have a smaller distance $dist(p,q)$ than the threshold $\epsilon$. Formally: 
% \[N_{\epsilon}(p)=\{q \in DS \mid dist(p,q) \leq \epsilon\}\]
\begin{align}
    N_{\epsilon}(p)=\{q \in DS \mid dist(p,q) \leq \epsilon\}
\end{align}

This measure alone however is not enough to fully cluster dense groups, since it would only register points inside the cluster (core points) and omit those at the border of the clusters (border points), since they have less points in their neighborhood. To include those border points too, \todo{cite style} \citeauthor{DBSCANEKSX96} introduce three more properties/relationships between points.
% \end{defn}

\subsubsection*{(Direct) Density-Reachability}
The direct density-reachability of a point $p$ from another point $q$ is fulfilled if:
% \begin{enumerate}
%     \item $p \in N_{\epsilon}(q)$
%     \item \label{eq:minptsreq}\(|N_{\epsilon}(q)|\geq MinPts\)
% \end{enumerate}
% \begin{flalign}
%     &p \in N_{\epsilon}(q)\label{eq:pinN}&\\\vspace{2mm}
%     &|N_{\epsilon}(q)|\geq MinPts\label{eq:minptsreq}&
% \end{flalign}
\begin{align}
    p \in N_{\epsilon}(q)\label{eq:pinN}\\
    |N_{\epsilon}(q)|\geq MinPts\label{eq:minptsreq}
\end{align}
where \autoref{eq:minptsreq} represents the condition for point \(q\) to be a core point. If a point \(p\) is in the $\epsilon$-Neighborhood of a core point $q$, but itself is not a core point, then point $p$ is labeled as a border point. 

\todor{original sounds way better}
A target point $p$ is density-reachable from origin point $q$, if there is a sequence of connecting points $p, c_1, \dotsc, c_n, q$ in between $p$ and $q$ which are directly density-reachable to its predecessor, e.g. $c_1$ is directly density-reachable from point $q$, $c_2$ is directly density-reachable from point $c_1$, \dots\todor{format} until $p$ is directly density-reachable from point $c_n$. This relation is obviously transitive, but only symmetric if both points of the relation are core points. In case of a border point as the origin there are not enough points in its neighborhood and therefore it does not have any density-reachable neighbor as a target to start with.

\todog{own fig}
\begin{figure}
    \centering
    \begin{minipage}[t]{.5\textwidth}
      \centering  
      \captionsetup{width=.9\linewidth}
      \includegraphics[width=.8\textwidth]{figures/directlydensityreachable.png}
      \captionof*{figure}{Border point p is directly density-reachable from core point q}
      \label{fig:test1}
    \end{minipage}%
    \begin{minipage}[t]{.5\textwidth}
      \centering
      \captionsetup{width=.9\linewidth}
      \includegraphics[width=.8\textwidth]{figures/density-reachable.png}
      \captionof*{figure}{p is not directly density-reachable, but density-reachable from q}
      \label{fig:test2}
    \end{minipage}
    \caption{Illustration of reachabilities wrt. $\epsilon$ and $MinPts = 3$, src: KDD-VL}
\end{figure}

\subsubsection*{Density-Connectedness}
The last relation between points is the density-connectedness. Two points $p$ and $q$ are density-connected, if there exists a point $c$, where both points $p$ and $q$ are density-reachable from point $c$. In contrast to the (direct) density-reachability, this relation is symmetric and even reflexive if both points $p$ and $q$ are density-reachable from each other. \todo{generate wider figure saving some space}
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/KMeansVSDBSCAN.pdf}
    \caption{Caption}
    \label{fig:kmeansdbscan}
\end{figure}

\vspace{5mm}

Given the previous defined relations we can now define the notion of clusters and noise in DBSCAN:

\subsubsection*{Clusters}
A dense cluster is the union of a set of density-connected core points $CP$ and the set of border points $BP$ which are density-reachable from $CP$.
Formally \todo{cite style}\citeauthor{DBSCANEKSX96} defines a DBSCAN-cluster as:

A dense cluster in data set $DS$ is a non-empty subset $C$ of points $p$ and $q$ if the following two conditions are fulfilled:
\todor{verbatim definition!}
% \begin{enumerate}
%     \item $\forall p, q$: if $p \in C$ and $q$ is density-reachable from $p$, then $q \in C$ (Maximality)
%     \item $\forall p, q \in C$: $p$ is density-connected to $q$ (Connectivity)
% \end{enumerate}
\begin{align}
    &\forall p, q: \text{if } p \in C \text{ and } q \text{ is density-reachable from } p \text{, then } q \in C \\
    &\forall p, q \in C: p \text{ is density-connected to }q
\end{align}

This definition of clustering therefore defines any cluster with \textit{at least} a density defined by the Epsilon-distance $\epsilon$ and the number of minimum points $MinPts$.

\subsubsection*{Noise}
Every point not belonging to any of the existing clusters $C_1, \dotsc, C_k$ in data set $DS$ are now considered as noise: 
\begin{align}
    noise = \{p \in  DS | \forall i : p \notin C_i\}
\end{align}
\subsubsection*{The DBSCAN-Algorithm}
One way to calculate the clusters using DBSCAN is as follows:

First select the first point $p$ of data set $DS$ and check its $\epsilon$-neighborhood. Save every neighbor into a seed list/queue $L$ for further evaluation and check if the neighborhood $N_{\epsilon}(p)$ has enough points to satisfy the $MinPts$-property. 
If it satisfies the property, then point $p$ is a core point and creates a new dense cluster. As long as the seed list is not empty every next point in the list is then classified as either part of the current cluster or noise.
However if $N_{\epsilon}(p)$ does not satisfy the $MinPts$-property, then point $p$ is prematurely labeled as noise. This status will be changed, if another point $q$ has $p$ in its $\epsilon$-neighborhood and itself is a core point, then it is relabeled as a border point of the current cluster.

Then the next point from the seed list $L$ is selected and the same procedure is applied again.
If the seed list $L$ is empty the current cluster is done and the next not previously observed point from $DS$ is evaluated and the whole cycle repeats again until there is no point in the data set $DS$ left.

\vspace{5mm}
\begin{algorithm}[H]
% \algsetup{linenosize=\tiny}
% \scriptsize
\SetAlgoLined
\KwData{data set $DS$; seed list $L$; current Cluster $C_i$; Noise $N$; $MinPts$; $\epsilon$}
\KwResult{A set $R$ of clusters $C_1,\dotsc,C_n$ and noise}
 o := first Point of $DS\setminus \{R \cup N\}$\;
 L := [o]\;
 i := 0\;
 \While{$DS\setminus \{R \cup N\}$ is not empty}{
    \While{L is not empty}{
        currentPoint := L.pop(0)\;
        L.push($N_{\epsilon}(currentPoint)$)\;
        \eIf{$|N_{\epsilon}(currentPoint)| > MinPts$}{
            $C_i$.add(currentPoint)\;
            $C_i$.addAllNeighbors($N_{\epsilon}(currentPoint)$)\;
            N.removeAllNeighbors($N_{\epsilon}(currentPoint)$)\;
        }{
            N.add(currentPoint)
        }
    }
    R.add($C_i$)\;
    o := first Point of $DS\setminus \{R \cup N\}$\;
    L := [o]\;
    i := i + 1\;
 }
 \caption{DBSCAN}
\end{algorithm}
\vspace{5mm}

This algorithm was the first approach to segmenting the original data space into subintervals of importance. However, there is a drawback with using DBSCAN as the first segmentation. DBSCAN relies on a global parameter setting which can only find clusters with approximately the same density or higher (c.f. \autoref{fig:DBSCANhierarchy}) and therefore would not be able to find the intrinsic clustering structure well if the underlying models of the global correlations created linear correlations with different densities. 

\begin{figure}
    \centering
    \includegraphics[width=.5\textwidth]{figures/DBSCANleastdensity.png}
    \caption{Caption ~\cite{opticsankerst1999optics}}
    \label{fig:DBSCANhierarchy}
\end{figure}

\subsection{OPTICS}\label{ssec:OPTICSindepth} \todob{check comment}%OPTICS ONLY wide range of epsranges, not different minpts, since reachability is dependent on minpts, it extends epsilon, if minpts is not satisfied
To tackle this problem, we replaced DBSCAN with a more versatile algorithm/extension called \acf{OPTICS}, which had the capabilities to not only detect clusters of different densities more accurately but also opens more possibilities for different approaches, e.g. a more hierarchical approach, for future work.
Although \ac{OPTICS} itself is not a clustering algorithm, it does create an ordering of the points in a database, which represents the density-based clustering structure of a wide range of $\epsilon$-parameter settings, with which it is possible to automatically create a more accurate clustering with different densities compared to \ac{DBSCAN}~\cite{opticsankerst1999optics}. To create such a consistent ordering \todo{cite style} \citeauthor{opticsankerst1999optics} defines the following two distances on top of the previously mentioned definitions of \autoref{ssec:DBSCANindepth}:
\vspace{5mm}

Given a point $p$ from the data set $DS$, an epsilon-distance $\epsilon$, the \todor{format} $\epsilon$-neighborhood of $p$ $N_{\epsilon}(p)$, the minimum number of points to be a cluster $MinPts$ and the distance measure from $p$ to its $MinPts$-nearest neighbors $MinPts$-$dist(p)$. Then:
\todo{DBSCAN with one whole cluster instead of 2 seperate}
\begin{figure}
    \centering
    \begin{minipage}{.47\textwidth}
      \centering
      \includegraphics[width=.8\textwidth]{figures/DSwithDBSCANbadBoundingBoxes.pdf}
      \captionof{figure}{A figure}
      \label{fig:baddbscan}
    \end{minipage}%
    \begin{minipage}{.53 \textwidth}
      \centering
      \includegraphics[width=.8\textwidth]{figures/DSwithOPTICSBoundingBoxes.pdf}
      \captionof{figure}{Another figure}
      \label{fig:goodoptics}
    \end{minipage}
\end{figure}

\subsubsection*{Core-Distance}
The core-distance of p describes the smallest  distance $\epsilon_{core}$ with an upper bound of $\epsilon$ such that the number of points in $N_{\epsilon_{core}}(p)$ is greater than $MinPts$, i.e.\ $\epsilon_{core}$ is the smallest epsilon-distance possible for $p$ to be a core point w.r.t.\ to $\epsilon$ and $MinPts$. If $\epsilon_{core}$ exceeds $\epsilon$, then the core-distance of $p$ is \textit{undefined}. Formally the core-distance of p is defined as:
\begin{align}
    core\text{-}dist_{\epsilon,MinPts}(p)=
    \begin{cases}
        \text{undefined} &\text{if } |N_\epsilon(p)| < MinPts\\
        MinPts\text{-}dist(p) &\text{otherwise}
    \end{cases}
\end{align}

\subsubsection*{Reachability-Distance}
The reachability-distance of point $p$ w.r.t.\ another point $q$ is the smallest possible $\epsilon_{reach}$-distance with a lower bound of $core-dist(q)$ such that p is directly-density reachable from q. If $core-dist(q)$ is \textit{undefined}, then $q$ is not a possible core point and the $reach-dist(p,q)$ is also \textit{undefined}. F
Formally the reachability-distance of $p$ w.r.t.\ $q$ is defined as:
\begin{align}
    reach\text{-}dist_{\epsilon,MinPts}(p,q)=
    \begin{cases}
        \text{undefined} &\text{if } |N_\epsilon(q)| < MinPts\\
        max(core\text{-}dist(q), dist(p,q) &\text{otherwise})
    \end{cases}
\end{align}

\subsubsection*{The OPTICS-Algorithm} %This first processed point sound weird
The OPTICS-algorithm maintains two data structures to create the ordering, one seed list $L$, which contains (point, reach-dist)-tuples and is ordered by the reach-dist, and the cluster ordering list $O$. 
To create this ordering, \ac{OPTICS} first initializes the ordering $O$ with a random point $o_{proc}$ of $DS$ and a reach-dist of $\infty$. A reachability-distance of $\infty$ from point $a$ w.r.t.\ point $b$ describes that $a$  is simply not contained in the $\epsilon$-neighborhood of $b$ and therefore connected to the dense cluster. Since $o_{proc}$ is the first point of the ordering it does not have a previous cluster to connect to and will never be in the reachability-distance of another point in the ordering. This first point in processing $o_{proc}$ now populates the ordered seed list $L$ with points $o_i$, belonging to the $\epsilon$-neighborhood $N_\epsilon(o_{proc})$, and its reachability-distance w.r.t.\ $o_{proc}$, i.e.\ $reach$-$dist(o_i, o_{proc})$. The further iterations now depend on the state of the seed list $L$. Whenever $L$ is not empty, each next point $o_{proc}$ chosen for processing is taken from the ordered seed list and inserted into the cluster ordering list $O$ with its respective reachability-distance. If $L$ is empty, there is no candidate left to be density-connected and a new $o_{proc}$ is randomly chosen from $DS$ and inserted into $O$ with a reachability-distance of $\infty$ again. The point in process $o_{proc}$ is then used again to populate the seed list. This loop continues until there are no unprocessed points in $DS$ left.



\begin{algorithm}[H]
% \algsetup{linenosize=\tiny}
% \scriptsize
\SetAlgoLined
\KwData{data set $DS$; seed list $L$ ordered by reach-dist(); cluster order $O$; min \# of pts for a cluster $MinPts$; Eps-distance $\epsilon$}
\KwResult{Ordering $O$ representing the density-based clustering structure}
 L := $\emptyset$\;
 \While{$DS\setminus \{L \cup O\}$ is not empty}{
    \eIf{$L$ is empty}{
        o := random Point of $DS\setminus \{L \cup O\}$\;
        $reach$-$dist$ := $\infty$\;
        $O$.append((o,$reach$-$dist$))\;
    }{
        (o, $saved\ reach$-$dist)$) := $L$.pop(0)\;
        $O$.append((o, $reach$-$dist(o)$))
    }
    \ForEach{$p \in N_\epsilon(o) \setminus \{$O$\}$}{
        $L$.update(($p$, $reach$-$dist(p, o)$))\;
    }
 }
 \caption{OPTICS}
\end{algorithm}

\todob{maybe chapter about reachability plots}

\subsection{Hough Transformation [2]}\label{ssec:houghindepth}
The initial purpose of the Hough-transform was a technique to detect colinear points~\cite{houghOriginal1962method} and has since then found various other applications in fields like image processing/analysis~\cite{rosenfeld1969picture,ballard1981generalizing}, computer vision~\cite{davies2004machine} and correlation clustering~\cite{CASHachtert2008robust}.
% \subsubsection{The basic idea}
The basic idea was the transformation of 2-dimensional points $p_i = (x_i,y_i)$ in data space $\mathcal{D} \subseteq \R^2$ with x and y axis to functions in the form of ${m_{p_i}(t_{p_i}) = \frac{y_i}{x_i} - \frac{t_{p_i}}{x_i}}$ in parameter space $\mathcal{P} \subseteq \R^2$, also known as Hough space, with t and m axis, where $m$ is the slope and $t$ is the y-intercept of a line passing through point $p_i$~\cite{illingworth1988survey}. It can be visualized by imagining that each point $p_i$ can be explained by an infinite number of concurrent lines $C$ defined by the functions 
\begin{align}
    {y_{p_i}(m,x) = m \cdot (x - x_i) + y_i}
\end{align}

If we only consider the value of $x=0$ we can still uniquely characterize each equation and instead of the y-value $y_{p_i}$ get the y-intersect $t$ w.r.t.\ slope $m$:
\begin{align}\label{eq:houghparamspace}
\begin{split}
y_{p_i}(m,0) 
&= m \cdot (0 - x_i) + y_i\\
&= -x_i \cdot m + y_i = t(m)
\end{split}\\
\label{eq:hougheq}
\Rightarrow m_{p_i}(t_{p_i}) &= \frac{y_i}{x_i} - \frac{t_{p_i}}{x_i}
\end{align}

A point $p_i = (x_i, y_i)$ in data space is then characterized by each possible $(m,t)$-setting of \autoref{eq:houghparamspace}. In parameter space these continuous $(m_{p_i},t_{p_i})$-settings are uniquely defined by the function in \autoref{eq:hougheq}. A point in data space therefore can be represented as a line in the $(m,t)$-parameter space (c.f.\ \autoref{fig:houghmxt}). \todob{graphing tool} %https://www.yworks.com/products/yed.

\begin{figure}
    \centering
    \includegraphics{figures/HoughMXT.pdf}
    \caption{Caption}
    \label{fig:houghmxt}
\end{figure}

However using the slope-intercept form $y = m \cdot x + t$ comes with a drawback, it can not show lines parallel to the y-axis. As a solution \citeauthor{duda1971use} propose the use of the \acf{hnf} for the concurrent lines instead. Their equations in data space change to the following form:
\begin{align}\label{eq:hnfangles}
    \begin{split}
        \delta_{p_i}(\theta_{p_i}) &= x_i \cos{\theta_{p_i}} + y_i \sin{\theta_{p_i}}\\
        &= f_{p_i}(\theta_{p_i})
    \end{split}
\end{align}

where $\delta$ denotes the shortest distance from the respective line to the origin and $\theta$ its respective angle. The parameter space therefore changes to a \mbox{$(\theta,\delta)$-parameter space} and points in data space are transformed to sinusoidal curves instead \todog{own fig}~\autoref{fig:TODOHOUGH}. Since the settings $(\theta,\delta)$ and $(\theta+k\pi,-2\delta)$ represent the same line, $\theta$ is restricted to the interval $[0,\pi)$. Given a point $p_i$ and all angles $\theta_{i,j}$, the distances $\delta_{i,j}$ can be calculated with \autoref{eq:hnfangles}. Hence the parameter space $\mathcal{P}$ can be bounded by $\mathcal{P} = [\delta_{min}, \delta_{max}]\times [0,\pi)$ where $\delta_{min}$ represents the global minimum and $\delta_{max}$ represents the global maximum of all functions $f$ w.r.t. all points. For easier illustration the angle $\theta$ can be transformed into a unit normal vector $\vec{n_0}$ (c.f.\ \autoref{eq:hnfunv}). \todo{hnf fig with $(\theta,\delta)$ and $(\vec{n},\delta)$}

\begin{align}
    \delta &= x_i \cos{\theta} + y_i \sin{\theta}\nonumber\\
    \text{substit}&\text{ute:}\ n_x = \cos{\theta},\ n_y = \sin{\theta} \nonumber\\
    \delta &= x_i n_x + y_i n_y\nonumber\\
    \label{eq:hnfunv}
    \delta &= \vec{n_0}\cdot\vec{x} 
\end{align}

\missingfigure{hnf visualizations in 2d}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/TODOHOUGHTHETADIST.png}
    \caption{\cite{CASHachtert2008global}}
    \label{fig:TODOHOUGH}
\end{figure}

Summarized this duality between data and parameter space results in the following properties:\label{ssec:properties}
\todor{choose a style}
\begin{property}\label{prop:hough1}
A point in data space corresponds to a (sinusoidal) function in the parameter space. 
\end{property}
\begin{property}\label{prop:hough2}
A point in parameter space corresponds to a linear function in the data space.
\end{property}
\begin{property}\label{prop:hough3}
Points lying on the same line in data space have a common intersection in parameter space.
\end{property}
\begin{property}\label{prop:hough4}
Points lying on the same (sinusoidal) function in parameter space correspond to lines through the same point in data space.
\end{property}

Since points in parameter space correspond to a linear function in data space and a curve in parameter space corresponds to a point in data space, we can deduct that intersections of curves in parameter space correspond to points lying on the same line in data space. Therefore through the transformation the problem of finding correlated points becomes a problem of finding points or regions of intersections of curves in parameter space. Another added benefit is the parameter space's independence of the locality assumption since regardless of the points locations in data space its functions intersections in parameter space represent points lying on the same line. \todor{wtf sentence} To solve this problem there are several approaches. For exact results looking for points with high intersections can be done by solving linear equation systems of the functions in parameter space. This however can quickly become computationally infeasible and does not cope for jitter, which causes disruptions in the exact intersections in parameter space. Static predefined grid-based approaches, like searching for 2-dimensional regions of interest with a predefined grid, or dynamic approaches, like done in \textcite{CASHachtert2008global} by splitting the 2d search space more efficiently, are some computationally less expensive solutions. An indepth explanation of one approach, namely \acf{CASH}, will be given in \fullref{ssec:CASH}.

\subsection{CASH [1]}\label{ssec:CASHindepth}
\todo{check n-dim Hough transform explanation with DK}
\acf{CASH} is a subspace clustering algorithm based on the principle of the Hough Transform which, in contrast to the axis-parallel algorithms \ac{SUBCLU}\cite{sublcukailing2004density} and \ac{CLIQUE}\cite{cliqueagrawal1998automatic}, can detect arbitrarily oriented subspaces (c.f.~\autoref{ssec:houghindepth}. \todor{sounds awful}
For arbitrary-dimensional data spaces $\mathcal{D} \subseteq \R^d$ \textcite{CASHachtert2008robust} reformulates the Hough transformation via a generalized description of spherical coordinates. A $d$-dimensional point/vector $x=(x_1,\dotsc,x_d)$ w.r.t.\ the given orthonormal basis $e_1,\dotsc,e_d$, can be described with $d-1$ independent angles $\alpha_1,\dotsc,\alpha_{d-1}$ and the norm/distance to origin of vector/point $x$. The following definitions are taken from \cite{CASHachtert2008robust}:

\subsubsection*{Spherical Coordinates}\label{def:spherecord}
\todor{verbatim definition!}
Let $e_i, 1 \leq i \leq d$, be an orthonormal basis in a $d$-dimensional feature space. Let $x=(x_1,\dotsc,x_d)$ be a $d$-dimensional vector on the hypersphere of radius $r$ with center at the origin. Let $u_i$ be the unit vector in the direction of the projection of $x$ onto the manifold spanned by $e_i,\dotsc,e_d$. For the $d-1$ independent angles $\alpha_1,\dotsc,\alpha_{d-1}$, let $\alpha_i$, $1 \leq i \leq d-1$, be the angle between $u_i$ and $e_i$. Then the generalized spherical coordinates of vector $x$ are defined by:
\begin{align}
    x_i = r \cdot (\prod_{j=1}^{i-1}\sin{\alpha_j}) \cdot \cos{\alpha_i}\text{, where } \alpha_d = 0.
\end{align}

Similar to \autoref{ssec:houghindepth} any $d$-dimensional point $p \in \mathcal{D}$ can be explained by an infinite number of hyperplanes containing $p$. These hyperplanes can be uniquely defined by the $d-1$ angles $\alpha_1,\dotsc,\alpha_{d-1}$, with $\alpha_i \in [0,\pi)$ representing the normal vector of the hyperplanes, and a fix point $o$ on the hyperplane. To acquire a point independent representation \textcite{CASHachtert2008robust} maps the angles $\alpha_i$ and point $p$ with the following parametrization function to the shortest distance $\delta$ of the hyperplane to the origin (c.f. \ref{eq:hnfangles}).

\subsubsection*{Parametrization Function}
Let $p = (p_1,\dotsc,p_d)$ be a $d$-dimensional unit vector specified by $d-1$ angles $\alpha_1,\dotsc,\alpha_{d-1}$ according to the \hyperref[def:spherecord]{previous definition \textit{``Spherical Coordinates''}}. Then the parametrization function $f_p:\R^{d-1} \rightarrow \R$ of vector $p$ denotes the distance of the hyperplane defined by the point $p$ and the normal vector $n$ to the origin:
\begin{align}
    \begin{split}
    f_p(\alpha_1,\dotsc,\alpha_{d-1}) &= \langle p,n \rangle \\
    &= \sum_{i=1}^d p_i \cdot (\prod_{j=1}^{i-1} \sin{\alpha_j}) \cdot \cos{\alpha_i} = \delta
    \end{split}
\end{align}

Given $d-1$ angles $\alpha_1,\dotsc,\alpha_{d-1}$ and shortest distances $\delta$ obtained by the parametrization function for a point $p_i = (x_1,\dotsc,x_d)$ and its infinite hyperplanes, each point $p$ in data space $\mathcal{D}$ can be mapped to a $d$-dimensional parameter space $\mathcal{P} \subseteq \R^d$ spanned by the previously mentioned parameters.
By the means of the parametrization function \textcite{CASHachtert2008robust} extend the properties of the original Hough transformation (c.f. \autoref{ssec:properties}) to $d$-dimensional data spaces and its corresponding parameter spaces: \todor{verbatim}
\begin{quoting}
\begin{property}
A point $p$ in data space $\mathcal{D} \subseteq \R^d$ is represented by a sinusoidal curve $f_p:\R^{d-1}\rightarrow\R$ in parameter space.
\end{property}
\begin{property}
A point $p' = (\alpha_1,\dotsc,\alpha_{d-1},\delta$ in parameter space $\mathcal{P} \subseteq R^d$ corresponds to a $(d-1)$-dimensional hyperplane in data space.
\end{property}
\begin{property}
Points that are located on a $(d-1)$-dimensional hyperplane in data space correspond to sinusoidal curves through a common point in parameter space.
\end{property}
\begin{property}
Points lying on the same sinusoidal curve in parameter space represent $(d-1)$-dimensional hyperplanes through the same point in data space.
\end{property}
\end{quoting}

% \begin{enumerate}
%     \item A point $p$ in data space $\mathcal{D} \subseteq \R^d$ is represented by a sinusoidal curve $f_p:\R^{d-1}\rightarrow\R$ in parameter space.
%     \item A point $p' = (\alpha_1,\dotsc,\alpha_{d-1},\delta$ in parameter space $\mathcal{P} \subseteq R^d$ corresponds to a $(d-1)$-dimensional hyperplane in data space.
%     \item Points that are located on a $(d-1)$-dimensional hyperplane in data space correspond to sinusoidal curves through a common point in parameter space.
%     \item Points lying on the same sinusoidal curve in parameter space represent $(d-1)$-dimensional hyperplanes through the same point in data space.
% \end{enumerate}
\missingfigure[]{3d example}

Boundaries of the parameter space $\mathcal{P}$ are extended to $\mathcal{P} = [\delta_{min}, \delta_{max}]\times [0,\pi)^{d-1}$ with $\delta_{min}$ and $\delta_{max}$ being the minimum and maximum of all functions $f_p$ for $\alpha \in [0,\pi)$ again: $[\delta_{min}, \delta_{max}] = [min_{p \in \mathcal{D}}(f_p(\alpha_p^{min})), max_{p \in \mathcal{D}}(f_p(\alpha_p^{max}))$, where $\alpha_p^{min}$ and $\alpha_p^{max}$ denote the minimum and maximum of function $f_p$. An in detail explanation is found in \textcite{CASHachtert2008robust}.
Applying the same concept of \autoref{ssec:houghindepth} the goal of finding $d$-dimensional points lying on the same hyperplane requires finding intersections of the corresponding $d$-dimensional curves in parameter space. Since an analytical solution of the intersections in multi-dimensional space is infeasible, the parameter space is scanned for $d$-dimensional regions/hypercuboids of a certain amount $m$ of intersections instead which additionally copes for jitter. Regions which fulfill the number of intersections are called \textit{dense grid cells}.

\subsubsection*{The CASH-Algorithm}
Given previously defined data representation in data and parameter space, the goal is finding dense grid cells in parameter space. However since the complexity of searching the parameter space with a predefined grid is exponential w.r.t.\ dimension $d$ it also quickly gets too computational expensive for higher dimensional spaces. \ac{CASH} tackles this problem by smartly dividing the search space. \citeauthor{CASHachtert2008robust} proposed the following search strategy:
\vspace{5mm}

The parameter space is successively divided by the axes in the static order given by $\delta,\alpha_1,\dotsc,\alpha_{d-1}$. After each split the hypercuboid with more intersections is divided at the next axis of the order. If both hypercuboids have the same amount of intersections, the first hypercuboid is selected (arbitrarily). The other hypercuboid is kept in a queue, which is ordered descendingly by the number of its intersecting curves/points. If regions in the queue have an equal amount of intersections, the smaller region is prefered. These splits and additions to the queue are done until both split hypercuboids have less than a minimum number of intersections $m$ to be considered as dense or a predefined depth $s$ has been reached and the hypercuboids fulfilling $m$ and are dense are considered as valid subspace clusters. These valid $(d-1)$-dimensional subspace clusters are then recursively evaluated by CASH again until the resulting found subspace clusters have a minimum dimensionality of $minDims$. If a subspace cluster is found, its intersecting curves are removed from any other hypercuboid in the queue, which is then updated and sorted according to its new status. If the number of intersections drops below $m$, the hypercuboid is removed from the queue. This procedure is done, until the queue is empty.

\begin{algorithm}[H]
% \algsetup{linenosize=\tiny}
% \scriptsize
\SetAlgoLined
\KwData{parameter space $\mathcal{P}$; queue $Q$ ordered by number of intersections and its volume; iterator $I$ of order of axis $[\delta,\alpha_1,\dotsc,\alpha_{d-1}]$; set of subspace clusters $C$; minimum number of intersections $m$; maximum number of splits $s$}
\KwResult{Set of clusters $C$ representing the subspaces found in $\PS$}
\SetKwFunction{CASH}{CASH}
\SetKwProg{Fn}{Function}{:}{\KwRet $C$}
Q := $\emptyset$\;     
h1 := first hypercuboid of split\;
h2 := second hypercuboid of split\;
h := $\PS$\tcp*{working hypercuboid}
\Fn{\CASH{$h$, $m$, $s$}}{
     \Do{$Q$ is not empty}{
        h1, h2 := h.splitAxis($I$.next())\;
        h := $arg\,max_{cube \in \{h1,h2\}}(cube.intersections)$\;
        Q.add(\{h1,h2\}$\setminus$ h)\;
        splitcounter := 1\;
        \While{$h.intersections > m$ and $splitcounter < s$}{
            h1, h2 := h.splitAxis($I$.next())\;
            splitcounter++\;
            h := $arg\,max_{cube \in \{h1,h2\}}(cube.intersections)$\;
            Q.add(\{h1,h2\}$\setminus$ h)\;
        }
        \If{$h.intersections > m$}{
            $C := C\ \cup$\ \CASH{h,m,s}\;
        }
        $I$.reset()\tcp*{reset Iterator for split of next $h$}
        $Q$.removeIntersectionsAndUpdate($C$)\;
    }
}

 \caption{CASH}
\end{algorithm}

The resulting set $C$ contains all $n$-dimensional subspace clusters with $n < d$ found within the constraints of minimum points in correlation $m$ and maximum splits $s$.
\todor{Check recursive CASH}

Summarized we now possess means to find density-based clusters and global subspace clusters in an original dataset $DS$. The density-based approach \ac{OPTICS} requires two parameters, minimum number of points $minPts$ and a maximum epsilon distance $\epsilon$, which essentially specifiy the minimum density allowed, to acquire a hierarchical view of density-connected clusters in $DS$. The subspace clustering approach \ac{CASH} also requires another two parameters, minimum number of points/intersections $m$ and number of splits $s$ to be considered as a subspace cluster.

\section{The Algorithm [5]}
With the previously mentioned algorithms we are already able to find arbitrarily shaped clusters with a density-based approach, and arbitrarily oriented subspace clusters with e.g. \ac{4C} or \ac{CASH}, however all of them are restricted to finding those clusters in a purely local or purely global fashion. E.g. none of the previously mentioned algorithms can handle scenarios as shown in 
\missingfigure[]{scenario zeigen}
Above Figures \todor{figure erstellen} depict settings, where the data contains multiple characteristic local correlations which however cannot be accurately described by the aforementioned methods alone. Figure 1 shows cross-shaped dense clusters which can be picked up by \ac{DBSCAN} or \ac{OPTICS}, however it does not deliver their correlations. As an extension \ac{4C} is able to detect the local correlations, however gets skewed results in the crosses due to the correlations orientation not being accurately representable by the eigenvectors of the \ac{PCA}. \ac{CASH} on the other hand could find the correlations. Figure 2 \todor{figure} shows multiple local settings of different  correlations. \ac{4C} is able to find the accurate local correlations, however can not categorize those local correlations as one global correlation\todor{naja an sich schon. mit richtigem parameter setting}. On the other hand CASH can only find the global correlation while ommitting the information of the gaps in data. Hierarchical subspace clustering methods like \ac{HiCO} and \ac{ERiC} are not helpful either, since their hierarchy is based on lower dimensional subspaces contained in subspaces and not subspaces composed of same dimensional subspaces. Either the subspace clustering method detects local correlations while missing the big picture or they do find global correlations while losing structural information of its particular correlation. This thesis proposes an algorithm that unifies local and global correlation clustering by evaluating \textit{locally dense} regions of interest with a correlation clustering method, e.g. \ac{CASH}, and combining those local intermediate correlations into a global correlation clustering to get a universal subspace clustering result. 

%Additionally it does not categorize both horizontal local correlations as one, even though they arguably could be interpreted that way.
% , not the big picture

\subsection{Partitioning Data into Dense Clusters}
% The first step we do is the partitioning of the data space into dense clusters via a density-based clustering approach like \ac{DBSCAN} or \ac{OPTICS}. 
%However like mentioned in \autoref{ssec:DBSCANindepth} and \autoref{ssec:OPTICSindepth}
Since we want to find \textit{local} linear correlations within \textit{global} linear correlations, we assume that the global linear correlation is composed of a set of its local linear correlations. This also means that points belonging to the same global linear correlation are also part of the composing local subclusters which have a similar inherent linear correlation as the global correlation. These subclusters have a continous distribution of density compared to the global correlation itself, since else it would have been split as well and counted as seperate local subclusters. Vice versa the local components are also disjunct to each other, since else there would be a connection to each other to create a bigger subcluster. Although it is not optimal even if there is an overlap of different global linear correlations and the subcluster contains multiple of them, it still can retain the information of its composing linear correlations.
We can therefore retrieve these local subclusters by searching the data for density-connected sets of points via a density-based clustering approach like \ac{DBSCAN} or \ac{OPTICS}. This comes with an additional benefit by automatically removing noise. 

However like mentioned in \autoref{ssec:DBSCANindepth} and \autoref{ssec:OPTICSindepth} \ac{DBSCAN} is not able to find clusters with different density and mixes multiple densities together into one cluster due to its global parameter setting only defining a minimum density. High variance in data densities can therefore strongly influence the result of \ac{DBSCAN}, either by making it harder to choose the appropriate parameters for a better clustering result or by having more variety in density in found clusters which would result in less conclusive detection of their linear correlations. 
To receive a more suited clustering we preferred the use of \ac{OPTICS}, which is able to detect clusters with different densities (c.f.~\autoref{ssec:OPTICSindepth}). Since we do not want to prune any information away we can generally use a very high epsilon-distance $\epsilon$ to create a detailed ordering and therefore only rely on $MinPts$ as a parameter. From this ordering we can extract clusters with a new parameter $reachdist_{thres}$ which represents the threshold at which the difference of neighboring reachability-distances of the ordering is considered to be a new cluster with different density.

Find dense clusters using DBSCAN --> elaborate on bounding boxes around dense areas

\subsection{Finding Linear Correlations}
Apply CASH onto dense Clusters [2]

\subsection{Stitching}

Assembly of local linear correlations [2]