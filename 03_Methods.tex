\chapter{Methods}\label{ch:Methods}
% In this chapter we first introduce necessary mathematical and algorithmic foundations to provide an in-depth view of our algorithm's core components and explain their relevance in the new approach. 
In this chapter we propose a novel approach/concept to create a universal clustering result by combining a local correlation clustering approach with global one. For this purpose we first introduce necessary mathematical and algorithmic foundations to provide an in-depth view of our algorithm's core components and then specifically elaborate on them further by giving a detailed explanation to its functionality and explaining their relevance to our concept. With this knowledge/expertise we offer a first\todor{bin mir nich sicher ob first tho} approach by combining mentioned components and visualizing the single steps to present a more intuitive view of the procedure. \todor{v2 in comments}

% In this chapter we propose a novel approach/concept to create a universal clustering result by combining a local correlation clustering approach with global one. For this purpose we first introduce necessary mathematical and algorithmic foundations to provide an in-depth view of our algorithm's core components. We then specifically elaborate on them further by giving a detailed explanation to its functionality and explaining their relevance to our concept and offer a first approach by combining mentioned components and visualizing the single steps to present a more intuitive view of the procedure. 


% In this chapter 
% Clustering in Arbitrary Subspaces based on the Hough transform (CASH) is cool, however its complexity is $\mathcal{O}(n^2)$.
% We try to do it better via using a density based approach to prune away irrelevant points and noise..

\section{Mathematical and Algorithmic Foundations}
\label{sec:Foundations}
Our Algorithm builds on top of two algorithms which both were build on two other well-known algorithms: \gls{optics}, which is closely related to \gls{dbscan}, and \gls{cash}, which is inherently based on the Hough Transform.
\subsection{DBSCAN - In Depth}
\label{ssec:DBSCANindepth}
\begin{figure}
    \centering
    \begin{minipage}{.47\textwidth}
      \centering
      \includegraphics[width=.8\textwidth]{figures/DSwithDBSCANBoundingBoxes.pdf}
      \captionof{figure}{A figure}
      \label{fig:test1}
    \end{minipage}%
    \begin{minipage}{.53 \textwidth}
      \centering
      \includegraphics[width=.8\textwidth]{figures/DBSCANwithNoise.pdf}
      \captionof{figure}{Another figure}
      \label{fig:test2}
    \end{minipage}
\end{figure}
To begin with we voted for the use of \gls{dbscan} to retrieve the dense intervals of the original data set. 
As mentioned before the algorithm \gls{dbscan} is categorized into the density-based approach of clustering. This means, that it considers a set of points connected through low proximity in a restricted area/space into similar clusters and thus can, in contrast to partitioning based approaches, create clusters of arbitrary shape (cf. \autoref{fig:kmeansdbscan}). Hence these clusters also can retain information more accurate information about the correlation of the data. Additionally it also directly eliminates noise and can therefore lower the computational cost of the future steps. Its basic principle is as follows:
A group of points is considered as a dense cluster when for each point of the cluster there are sufficiently other points in its neighborhood~\cite{DBSCANEKSX96}.

To understand, how this algorithm works we first have to introduce some terminology.
Given a data set $DS$ of data objects $o$ with a dimensionality of $N$, a distance measure $dist(p_1,p_2))$ and two new parameters, the Epsilon distance $\epsilon$ and the minimum number of points to be a cluster $MinPts$, \todo{cite style} \citeauthor{DBSCANEKSX96} defines:

\subsubsection*{$\epsilon$-Neighborhood}
% \begin{defn}
The $\epsilon$-Neighborhood $N_{\epsilon}(p)$ of a point $p$ is defined by the set of points $q$ which have a smaller distance $dist(p,q)$ than the threshold $\epsilon$. Formally: 
% \[N_{\epsilon}(p)=\{q \in DS \mid dist(p,q) \leq \epsilon\}\]
\begin{align}
    N_{\epsilon}(p)=\{q \in DS \mid dist(p,q) \leq \epsilon\}
\end{align}

This measure alone however is not enough to fully cluster dense groups, since it would only register points inside the cluster (core points) and omit those at the border of the clusters (border points), since they have less points in their neighborhood. To include those border points too, \todo{cite style} \citeauthor{DBSCANEKSX96} introduce three more properties/relationships between points.
% \end{defn}

\subsubsection*{(Direct) Density-Reachability}
The direct density-reachability of a point $p$ from another point $q$ is fulfilled if:
% \begin{enumerate}
%     \item $p \in N_{\epsilon}(q)$
%     \item \label{eq:minptsreq}\(|N_{\epsilon}(q)|\geq MinPts\)
% \end{enumerate}
% \begin{flalign}
%     &p \in N_{\epsilon}(q)\label{eq:pinN}&\\\vspace{2mm}
%     &|N_{\epsilon}(q)|\geq MinPts\label{eq:minptsreq}&
% \end{flalign}
\begin{align}
    p \in N_{\epsilon}(q)\label{eq:pinN}\\
    |N_{\epsilon}(q)|\geq MinPts\label{eq:minptsreq}
\end{align}
where \autoref{eq:minptsreq} represents the condition for point \(q\) to be a core point. If a point \(p\) is in the $\epsilon$-Neighborhood of a core point $q$, but itself is not a core point, then point $p$ is labeled as a border point. 

\todor{original sounds way better}
A target point $p$ is density-reachable from origin point $q$, if there is a sequence of connecting points $p, c_1, \dotsc, c_n, q$ in between $p$ and $q$ which are directly density-reachable to its predecessor, e.g. $c_1$ is directly density-reachable from point $q$, $c_2$ is directly density-reachable from point $c_1$, \dots\todor{format} until $p$ is directly density-reachable from point $c_n$. This relation is obviously transitive, but only symmetric if both points of the relation are core points. In case of a border point as the origin there are not enough points in its neighborhood and therefore it does not have any density-reachable neighbor as a target to start with.

\todog{own fig}
\begin{figure}
    \centering
    \begin{minipage}[t]{.5\textwidth}
      \centering  
      \captionsetup{width=.9\linewidth}
      \includegraphics[width=.8\textwidth]{figures/directlydensityreachable.png}
      \captionof*{figure}{Border point p is directly density-reachable from core point q}
      \label{fig:test1}
    \end{minipage}%
    \begin{minipage}[t]{.5\textwidth}
      \centering
      \captionsetup{width=.9\linewidth}
      \includegraphics[width=.8\textwidth]{figures/density-reachable.png}
      \captionof*{figure}{p is not directly density-reachable, but density-reachable from q}
      \label{fig:test2}
    \end{minipage}
    \caption{Illustration of reachabilities wrt. $\epsilon$ and $MinPts = 3$, src: KDD-VL}
\end{figure}

\subsubsection*{Density-Connectedness}
The last relation between points is the density-connectedness. Two points $p$ and $q$ are density-connected, if there exists a point $c$, where both points $p$ and $q$ are density-reachable from point $c$. In contrast to the (direct) density-reachability, this relation is symmetric and even reflexive if both points $p$ and $q$ are density-reachable from each other. \todo{generate wider figure saving some space}
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/KMeansVSDBSCAN.pdf}
    \caption{Caption}
    \label{fig:kmeansdbscan}
\end{figure}

\vspace{5mm}

Given the previous defined relations we can now define the notion of clusters and noise in \gls{dbscan}:

\subsubsection*{Density-based Clusters}
A dense cluster is the union of a set of density-connected core points $CP$ and the set of border points $BP$ which are density-reachable from $CP$.
Formally \todo{cite style}\citeauthor{DBSCANEKSX96} defines a \gls{dbscan}-cluster as:

A dense cluster in data set $DS$ is a non-empty subset $C^{DB} \subseteq DS$ of points $p$ and $q$ if the following two conditions are fulfilled:
\todor{verbatim definition!}
% \begin{enumerate}
%     \item $\forall p, q$: if $p \in C$ and $q$ is density-reachable from $p$, then $q \in C$ (Maximality)
%     \item $\forall p, q \in C$: $p$ is density-connected to $q$ (Connectivity)
% \end{enumerate}
\begin{align}
    &\forall p, q: \text{if } p \in C^{DB} \text{ and } q \text{ is density-reachable from } p \text{, then } q \in C^{DB} \\
    &\forall p, q \in C^{DB}: p \text{ is density-connected to }q
\end{align}

This definition of clustering therefore defines any cluster with \textit{at least} a density defined by the Epsilon-distance $\epsilon$ and the number of minimum points $MinPts$.

\subsubsection*{Density-based Noise}
Every point not belonging to any of the existing clusters $C^{DB}_1, \dotsc, C^{DB}_k$ in data set $DS$ are now considered as noise: 
\begin{align}
    noise^{DB} = \{p \in  DS | \forall i : p \notin C^{DB}_i\}
\end{align}
\todob{transition?}

\subsubsection*{The DBSCAN-Algorithm}
One way to calculate the clusters using \gls{dbscan} is as follows:

First select the first point $p$ of data set $DS$ and check its $\epsilon$-neighborhood. Save every neighbor into a seed list/queue $L$ for further evaluation and check if the neighborhood $N_{\epsilon}(p)$ has enough points to satisfy the $MinPts$-property. 
If it satisfies the property, then point $p$ is a core point and creates a new dense cluster. As long as the seed list is not empty every next point in the list is then classified as either part of the current cluster or noise.
However if $N_{\epsilon}(p)$ does not satisfy the $MinPts$-property, then point $p$ is prematurely labeled as noise. This status will be changed, if another point $q$ has $p$ in its $\epsilon$-neighborhood and itself is a core point, then it is relabeled as a border point of the current cluster.

Then the next point from the seed list $L$ is selected and the same procedure is applied again.
If the seed list $L$ is empty the current cluster is done and the next not previously observed point from $DS$ is evaluated and the whole cycle repeats again until there is no point in the data set $DS$ left.

\vspace{5mm}
\begin{algorithm}[H]
% \algsetup{linenosize=\tiny}
% \scriptsize
\SetAlgoLined
% \DontPrintSemicolon
\KwData{data set $DS$; seed list $L$; current Cluster $C^{DB}_i$; Noise $N^{DB}$; $MinPts$; $\epsilon$}
\KwResult{A set $R$ of clusters $C^{DB}_1,\dotsc,C^{DB}_n$ and noise $N^{DB}$}
 o := first Point of $DS\setminus \{R \cup N\}$\;
 L := [o]\;
 i := 0\;
 \While{$DS\setminus \{R \cup N\}$ is not empty}{
    \While{L is not empty}{
        currentPoint := L.pop(0)\;
        L.push($N_{\epsilon}(currentPoint)$)\;
        \eIf{$|N_{\epsilon}(currentPoint)| > MinPts$}{
            $C^{DB}_i$.add(currentPoint)\;
            $C^{DB}_i$.addAllNeighbors($N_{\epsilon}(currentPoint)$)\;
            N.removeAllNeighbors($N_{\epsilon}(currentPoint)$)\;
        }{
            N.add(currentPoint)
        }
    }
    R.add($C^{DB}_i$)\;
    o := first Point of $DS\setminus \{R \cup N\}$\;
    L := [o]\;
    i := i + 1\;
 }
 \caption{DBSCAN}
\end{algorithm}
\vspace{5mm}

This algorithm was the first approach to segmenting the original data space into subintervals of importance. However, there is a drawback with using \gls{dbscan} as the first segmentation. \gls{dbscan} relies on a global parameter setting which can only find clusters with approximately the same density or higher (c.f. \autoref{fig:DBSCANhierarchy}) and therefore would not be able to find the intrinsic clustering structure well if the underlying models of the global correlations created linear correlations with different densities. 

\begin{figure}
    \centering
    \includegraphics[width=.5\textwidth]{figures/DBSCANleastdensity.png}
    \caption{Caption ~\cite{opticsankerst1999optics}}
    \label{fig:DBSCANhierarchy}
\end{figure}

\subsection{OPTICS - In Depth}\label{ssec:OPTICSindepth} \todob{check comment}%OPTICS ONLY wide range of epsranges, not different minpts, since reachability is dependent on minpts, it extends epsilon, if minpts is not satisfied
To tackle the detection of different densities, we replaced \gls{dbscan} with a more versatile algorithm/extension called \textit{\acrfull{optics}}, which had the capabilities to not only detect clusters of varying densities more accurately, but also opens more possibilities for different further approaches for future work, e.g. a more extensive (bottom-up/hierarchical) approach by evaluating more density-settings at once based on different $\epsilon$-ranges.
Although \gls{optics} itself is not a clustering algorithm, it does create an ordering of the points in a database, which represents the density-based clustering structure of a wide range of $\epsilon$-parameter settings, with which it is possible to automatically create a more accurate clustering with different densities compared to \gls{dbscan}~\cite{opticsankerst1999optics}. To create such a consistent ordering \todo{cite style} \citeauthor{opticsankerst1999optics} defines the following two distances on top of the previously mentioned definitions of \autoref{ssec:DBSCANindepth}:
\vspace{5mm}

Given a point $p$ from the data set $DS$, an epsilon-distance $\epsilon$, the \todor{format} $\epsilon$-neighborhood of $p$ $N_{\epsilon}(p)$, the minimum number of points to be a cluster $MinPts$ and the distance measure from $p$ to its $MinPts$-nearest neighbors $MinPts$-$dist(p)$. Then:
\todo{DBSCAN with one whole cluster instead of 2 separate}
\todor{Figure without bounding box}
\begin{figure}
    \centering
    \begin{minipage}{.47\textwidth}
      \centering
      \includegraphics[width=.8\textwidth]{figures/DSwithDBSCANbadBoundingBoxes.pdf}
      \captionof{figure}{A figure}
      \label{fig:baddbscan}
    \end{minipage}%
    \begin{minipage}{.53 \textwidth}
      \centering
      \includegraphics[width=.8\textwidth]{figures/DSwithOPTICSBoundingBoxes.pdf}
      \captionof{figure}{Another figure}
      \label{fig:goodoptics}
    \end{minipage}
\end{figure}

\subsubsection*{Core-Distance}
The core-distance of p describes the smallest  distance $\epsilon_{core}$ with an upper bound of $\epsilon$ such that the number of points in $N_{\epsilon_{core}}(p)$ is greater than $MinPts$, i.e.\ $\epsilon_{core}$ is the smallest epsilon-distance possible for $p$ to be a core point w.r.t.\ to $\epsilon$ and $MinPts$. If $\epsilon_{core}$ exceeds $\epsilon$, then the core-distance of $p$ is \textit{undefined}. Formally the core-distance of p is defined as:
\begin{align}
    core\text{-}dist_{\epsilon,MinPts}(p)=
    \begin{cases}
        \text{undefined} &\text{if } |N_\epsilon(p)| < MinPts\\
        MinPts\text{-}dist(p) &\text{otherwise}
    \end{cases}
\end{align}

\subsubsection*{Reachability-Distance}
The reachability-distance of point $p$ w.r.t.\ another point $q$ is the smallest possible $\epsilon_{reach}$-distance with a lower bound of $core-dist(q)$ such that p is directly-density reachable from q. If $core-dist(q)$ is \textit{undefined}, then $q$ is not a possible core point and the $reach-dist(p,q)$ is also \textit{undefined}. F
Formally the reachability-distance of $p$ w.r.t.\ $q$ is defined as:
\begin{align}
    reach\text{-}dist_{\epsilon,MinPts}(p,q)=
    \begin{cases}
        \text{undefined} &\text{if } |N_\epsilon(q)| < MinPts\\
        max(core\text{-}dist(q), dist(p,q) &\text{otherwise})
    \end{cases}
\end{align}

\subsubsection*{The OPTICS-Algorithm} %This first processed point sound weird
The \gls{optics}-algorithm maintains two data structures to create the ordering, one seed list $L$, which contains (point, reach-dist)-tuples and is ordered by the reach-dist, and the cluster ordering list $O$. 
To create this ordering, \gls{optics} first initializes the ordering $O$ with a random point $o_{proc}$ of $DS$ and a reach-dist of $\infty$. A reachability-distance of $\infty$ from point $a$ w.r.t.\ point $b$ describes that $a$  is simply not contained in the $\epsilon$-neighborhood of $b$ and therefore connected to the dense cluster. Since $o_{proc}$ is the first point of the ordering it does not have a previous cluster to connect to and will never be in the reachability-distance of another point in the ordering. This first point in processing $o_{proc}$ now populates the ordered seed list $L$ with points $o_i$, belonging to the $\epsilon$-neighborhood $N_\epsilon(o_{proc})$, and its reachability-distance w.r.t.\ $o_{proc}$, i.e.\ $reach$-$dist(o_i, o_{proc})$. The further iterations now depend on the state of the seed list $L$. Whenever $L$ is not empty, each next point $o_{proc}$ chosen for processing is taken from the ordered seed list and inserted into the cluster ordering list $O$ with its respective reachability-distance. If $L$ is empty, there is no candidate left to be density-connected and a new $o_{proc}$ is randomly chosen from $DS$ and inserted into $O$ with a reachability-distance of $\infty$ again. The point in process $o_{proc}$ is then used again to populate the seed list. This loop continues until there are no unprocessed points in $DS$ left.



\begin{algorithm}[H]
% \algsetup{linenosize=\tiny}
% \scriptsize
\SetAlgoLined
\KwData{data set $DS$; seed list $L$ ordered by reach-dist(); cluster order $O$; min \# of pts for a cluster $MinPts$; Eps-distance $\epsilon$}
\KwResult{Ordering $O$ representing the density-based clustering structure}
 L := $\emptyset$\;
 \While{$DS\setminus \{L \cup O\}$ is not empty}{
    \eIf{$L$ is empty}{
        o := random Point of $DS\setminus \{L \cup O\}$\;
        $reach$-$dist$ := $\infty$\;
        $O$.append((o,$reach$-$dist$))\;
    }{
        (o, $saved\ reach$-$dist)$) := $L$.pop(0)\;
        $O$.append((o, $reach$-$dist(o)$))
    }
    \ForEach{$p \in N_\epsilon(o) \setminus \{$O$\}$}{
        $L$.update(($p$, $reach$-$dist(p, o)$))\;
    }
 }
 \caption{OPTICS}
\end{algorithm}

\todob{maybe chapter about reachability plots}

\subsection{Hough Transformation - In Depth}\label{ssec:houghindepth}
The initial purpose of the Hough transform was a technique to detect colinear points~\cite{houghOriginal1962method} and has since then found various other applications in fields like image processing/analysis~\cite{rosenfeld1969picture,ballard1981generalizing}, computer vision~\cite{davies2004machine} and correlation clustering~\cite{CASHachtert2008robust}.
% \subsubsection{The basic idea}
The basic idea was the transformation of 2-dimensional points $p_i = (x_i,y_i)$ in data space $\mathcal{D} \subseteq \R^2$ with x and y axis to functions in the form of ${m_{p_i}(t_{p_i}) = \frac{y_i}{x_i} - \frac{t_{p_i}}{x_i}}$ in parameter space $\mathcal{P} \subseteq \R^2$, also known as Hough space, with t and m axis, where $m$ is the slope and $t$ is the y-intercept of a line passing through point $p_i$~\cite{illingworth1988survey}. It can be visualized by imagining that each point $p_i$ can be explained by an infinite number of concurrent lines $C$ defined by the functions 
\begin{align}
    {y_{p_i}(m,x) = m \cdot (x - x_i) + y_i}
\end{align}

If we only consider the value of $x=0$ we can still uniquely characterize each equation and instead of the y-value $y_{p_i}$ get the y-intersect $t$ w.r.t.\ slope $m$:
\begin{align}\label{eq:houghparamspace}
\begin{split}
y_{p_i}(m,0) 
&= m \cdot (0 - x_i) + y_i\\
&= -x_i \cdot m + y_i = t(m)
\end{split}\\
\label{eq:hougheq}
\Rightarrow m_{p_i}(t_{p_i}) &= \frac{y_i}{x_i} - \frac{t_{p_i}}{x_i}
\end{align}

A point $p_i = (x_i, y_i)$ in data space is then characterized by each possible $(m,t)$-setting of \autoref{eq:houghparamspace}. In parameter space these continuous $(m_{p_i},t_{p_i})$-settings are uniquely defined by the function in \autoref{eq:hougheq}. A point in data space therefore can be represented as a line in the $(m,t)$-parameter space (c.f.\ \autoref{fig:houghmxt}). \todob{graphing tool} %https://www.yworks.com/products/yed.

\begin{figure}
    \centering
    \includegraphics{figures/HoughMXT.pdf}
    \caption{Caption}
    \label{fig:houghmxt}
\end{figure}

However using the slope-intercept form $y = m \cdot x + t$ comes with a drawback, it can not show lines parallel to the y-axis. As a solution \citeauthor{duda1971use} propose the use of the \gls{hnf} for the concurrent lines instead. Their equations in data space change to the following form:
\begin{align}\label{eq:hnfangles}
    \begin{split}
        \delta_{p_i}(\theta_{p_i}) &= x_i \cos{\theta_{p_i}} + y_i \sin{\theta_{p_i}}\\
        &= f_{p_i}(\theta_{p_i})
    \end{split}
\end{align}

where $\delta$ denotes the shortest distance from the respective line to the origin and $\theta$ its respective angle. The parameter space therefore changes to a \mbox{$(\theta,\delta)$-parameter space} and points in data space are transformed to sinusoidal curves instead \todog{own fig}~\autoref{fig:TODOHOUGH}. Since the settings $(\theta,\delta)$ and $(\theta+k\pi,-2\delta)$ represent the same line, $\theta$ is restricted to the interval $[0,\pi)$. Given a point $p_i$ and all angles $\theta_{i,j}$, the distances $\delta_{i,j}$ can be calculated with \autoref{eq:hnfangles}. Hence the parameter space $\mathcal{P}$ can be bounded by $\mathcal{P} = [\delta_{min}, \delta_{max}]\times [0,\pi)$ where $\delta_{min}$ represents the global minimum and $\delta_{max}$ represents the global maximum of all functions $f$ w.r.t. all points. For easier illustration the angle $\theta$ can be transformed into a unit normal vector $\vec{n_0}$ (c.f.\ \autoref{eq:hnfunv}). \todo{hnf fig with $(\theta,\delta)$ and $(\vec{n},\delta)$}

\begin{align}
    \delta &= x_i \cos{\theta} + y_i \sin{\theta}\nonumber\\
    \text{substit}&\text{ute:}\ n_x = \cos{\theta},\ n_y = \sin{\theta} \nonumber\\
    \delta &= x_i n_x + y_i n_y\nonumber\\
    \label{eq:hnfunv}
    \delta &= \vec{n_0}\cdot\vec{x} 
\end{align}

\begin{figure}
    \centering
    % \includegraphics{}
    \missingfigure{hnf visualizations in 2d}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/TODOHOUGHTHETADIST.png}
    \caption{\cite{CASHachtert2008global}}
    \label{fig:TODOHOUGH}
\end{figure}

Summarized this duality between data and parameter space results in the following properties:\label{ssec:properties}
\todor{choose a style}
\begin{property}\label{prop:hough1}
A point in data space corresponds to a (sinusoidal) function in the parameter space. 
\end{property}
\begin{property}\label{prop:hough2}
A point in parameter space corresponds to a linear function in the data space.
\end{property}
\begin{property}\label{prop:hough3}
Points lying on the same line in data space have a common intersection in parameter space.
\end{property}
\begin{property}\label{prop:hough4}
Points lying on the same (sinusoidal) function in parameter space correspond to lines through the same point in data space.
\end{property}

Since points in parameter space correspond to a linear function in data space and a curve in parameter space corresponds to a point in data space, we can deduct that intersections of curves in parameter space correspond to points lying on the same line in data space. Therefore through the transformation the problem of finding correlated points becomes a problem of finding points or regions of intersections of curves in parameter space. Another added benefit is the parameter space's independence of the locality assumption since regardless of the points locations in data space its functions intersections in parameter space represent points lying on the same line. \todor{wtf sentence} To solve this problem there are several approaches. For exact results looking for points with high intersections can be done by solving linear equation systems of the functions in parameter space. This however can quickly become computationally infeasible and does not cope for jitter, which causes disruptions in the exact intersections in parameter space. Static predefined grid-based approaches, like searching for 2-dimensional regions of interest with a predefined grid/accumulator using a voting scheme, or dynamic approaches, like done in \textcite{CASHachtert2008global} by splitting the 2d search space more efficiently, are some computationally less expensive solutions. An in-depth explanation of one approach, namely \acrfull{cash}, will be given in \fullref{ssec:CASHindepth}.

\subsection{CASH - In Depth}\label{ssec:CASHindepth}
\todo{check n-dim Hough transform explanation with DK}
\acrfull{cash} is a subspace clustering algorithm based on the principle of the Hough Transform which, in contrast to the axis-parallel algorithms \gls{subclu}\cite{sublcukailing2004density} and \gls{clique}\cite{cliqueagrawal1998automatic}, can detect arbitrarily oriented subspaces (c.f.~\autoref{ssec:houghindepth}. \todor{sounds awful}
For arbitrary-dimensional data spaces $\mathcal{D} \subseteq \R^d$ \textcite{CASHachtert2008robust} reformulates the Hough transformation via a generalized description of spherical coordinates. A $d$-dimensional point/vector $x=(x_1,\dotsc,x_d)$ w.r.t.\ the given orthonormal basis $e_1,\dotsc,e_d$, can be described with $d-1$ independent angles $\alpha_1,\dotsc,\alpha_{d-1}$ and the norm/distance to origin of vector/point $x$. The following definitions are taken from \cite{CASHachtert2008robust}:

\subsubsection*{Spherical Coordinates}\label{def:spherecord}
\todor{verbatim definition!}
Let $e_i, 1 \leq i \leq d$, be an orthonormal basis in a $d$-dimensional feature space. Let $x=(x_1,\dotsc,x_d)$ be a $d$-dimensional vector on the hypersphere of radius $r$ with center at the origin. Let $u_i$ be the unit vector in the direction of the projection of $x$ onto the manifold spanned by $e_i,\dotsc,e_d$. For the $d-1$ independent angles $\alpha_1,\dotsc,\alpha_{d-1}$, let $\alpha_i$, $1 \leq i \leq d-1$, be the angle between $u_i$ and $e_i$. Then the generalized spherical coordinates of vector $x$ are defined by:
\begin{align}
    x_i = r \cdot (\prod_{j=1}^{i-1}\sin{\alpha_j}) \cdot \cos{\alpha_i}\text{, where } \alpha_d = 0.
\end{align}

Similar to \autoref{ssec:houghindepth} any $d$-dimensional point $p \in \mathcal{D}$ can be explained by an infinite number of hyperplanes containing $p$. These hyperplanes can be uniquely defined by the $d-1$ angles $\alpha_1,\dotsc,\alpha_{d-1}$, with $\alpha_i \in [0,\pi)$ representing the normal vector of the hyperplanes, and a fix point $o$ on the hyperplane. To acquire a point independent representation \textcite{CASHachtert2008robust} maps the angles $\alpha_i$ and point $p$ with the following parametrization function to the shortest distance $\delta$ of the hyperplane to the origin (c.f. \ref{eq:hnfangles}).

\subsubsection*{Parametrization Function}
Let $\vec{p} = (p_1,\dotsc,p_d)$ be a $d$-dimensional vector and $\vec{n_0} = (n_1,\dotsc,n_d)$ be a $d$-dimensional unit vector specified by $d-1$ angles $\alpha_1,\dotsc,\alpha_{d-1}$ according to the \hyperref[def:spherecord]{previous definition \textit{``Spherical Coordinates''}}. Then the parametrization function $f_{\vec{p}}:\R^{d-1} \rightarrow \R$ of vector $\vec{p}$ denotes the distance of the hyperplane defined by the point $p$ and the normal vector $\vec{n}$ to the origin:
\begin{align}
    \begin{split}
            f_{\vec{p}}(\alpha_1,\dotsc,\alpha_{d-1}) &= \langle \vec{p},\vec{n_0} \rangle\\
    &= \sum_{i=1}^d p_i \cdot (\prod_{j=1}^{i-1} \sin{\alpha_j}) \cdot \cos{\alpha_i}  = \delta
    \end{split}
\end{align}

Given $d-1$ angles $\alpha_1,\dotsc,\alpha_{d-1}$ and shortest distance $\delta$ obtained by the parametrization function for a point $p_i = (x_1,\dotsc,x_d)$ and its infinite hyperplanes, each point $p$ in data space $\mathcal{D}$ can be mapped to a $d$-dimensional parameter space $\mathcal{P} \subseteq \R^d$ spanned by the previously mentioned parameters, again creating a function defining a hyperplane:
\begin{align}
    \begin{split}
        \delta &= x_1 \cdot n_{0,1} + \dots + x_d \cdot n_{0,d}\\
        &= \langle \vec{x},\vec{n_0} \rangle\label{eq:highhnf}
    \end{split}
\end{align}
Similar to \autoref{ssec:houghindepth} the resulting hyperplane in parameter space can be transformed between  cartesian representation using a unit vector $\vec{n_0}$ and polar representation based on angles $\alpha_1,\dotsc,\alpha_{d-1}$. In the process of this thesis, we primarily use the cartesian form of the \gls{hnf} (c.f. \autoref{eq:highhnf}).


By the means of the parametrization function \textcite{CASHachtert2008robust} extend the properties of the original Hough transformation (c.f. \autoref{ssec:properties}) to $d$-dimensional data spaces and its corresponding parameter spaces: \todor{verbatim}
\begin{quoting}
\begin{property}
A point $p$ in data space $\mathcal{D} \subseteq \R^d$ is represented by a sinusoidal curve $f_p:\R^{d-1}\rightarrow\R$ in parameter space.
\end{property}
\begin{property}
A point $p' = (\alpha_1,\dotsc,\alpha_{d-1},\delta$ in parameter space $\mathcal{P} \subseteq R^d$ corresponds to a $(d-1)$-dimensional hyperplane in data space.
\end{property}
\begin{property}
Points that are located on a $(d-1)$-dimensional hyperplane in data space correspond to sinusoidal curves through a common point in parameter space.
\end{property}
\begin{property}
Points lying on the same sinusoidal curve in parameter space represent $(d-1)$-dimensional hyperplanes through the same point in data space.
\end{property}
\end{quoting}

% \begin{enumerate}
%     \item A point $p$ in data space $\mathcal{D} \subseteq \R^d$ is represented by a sinusoidal curve $f_p:\R^{d-1}\rightarrow\R$ in parameter space.
%     \item A point $p' = (\alpha_1,\dotsc,\alpha_{d-1},\delta$ in parameter space $\mathcal{P} \subseteq R^d$ corresponds to a $(d-1)$-dimensional hyperplane in data space.
%     \item Points that are located on a $(d-1)$-dimensional hyperplane in data space correspond to sinusoidal curves through a common point in parameter space.
%     \item Points lying on the same sinusoidal curve in parameter space represent $(d-1)$-dimensional hyperplanes through the same point in data space.
% \end{enumerate}
\missingfigure[]{3d example}

Boundaries of the parameter space $\mathcal{P}$ are extended to $\mathcal{P} = [\delta_{min}, \delta_{max}]\times [0,\pi)^{d-1}$ with $\delta_{min}$ and $\delta_{max}$ being the minimum and maximum of all functions $f_p$ for $\alpha \in [0,\pi)$ again: $[\delta_{min}, \delta_{max}] = [min_{p \in \mathcal{D}}(f_p(\alpha_p^{min})), max_{p \in \mathcal{D}}(f_p(\alpha_p^{max}))$, where $\alpha_p^{min}$ and $\alpha_p^{max}$ denote the minimum and maximum of function $f_p$. An in detail explanation is found in \textcite{CASHachtert2008robust}.
Applying the same concept of \autoref{ssec:houghindepth} the goal of finding $d$-dimensional points lying on the same hyperplane requires finding intersections of the corresponding $d$-dimensional curves in parameter space. Since an analytical solution of the intersections in multi-dimensional space is infeasible, the parameter space is scanned for $d$-dimensional regions/hypercuboids of a certain amount $m$ of intersections instead which additionally copes for jitter. Regions which fulfill the number of intersections are called \textit{dense grid cells}.

\subsubsection*{Correlation Cluster}\todor{check definition with dk}
Since we have to distinguish correlation clusters from density-based clusters, we hereby define a notion of correlation clusters.

Correlation in a classical sense is a measure of relationship between two variables/features, but can also be extended to relations between multiple features. Commonly the relation refers to a linear relationship, but can also be non-linear. Since we want to cluster these correlations not only by orientation, but also by positioning, we define a specifically located correlation $r_{l,f,t}$, with $l$ being an unambiguous location as the source of the correlation, $f$ being a set of predictor features and $t$ being the target/correlating feature, as a hyperplane $h(t,f,l)$. In case of \gls{cash} a localized correlation/hyperplane is defined by the \gls{hnf}(c.f. \autoref{eq:highhnf}) with $f$ and $t$ being embedded in the orientation $n_{0,i} \in \{f,t\}$ and distance $\delta$ and $l$ only being embedded in the latter one. 
\begin{align}
    \delta(t,f,l) = \langle \vec{x},\vec{n_0}(f,t) \rangle
\end{align}
\textit{Correlation clusters} refer to the grouping of observations/data objects within a certain interval in data space which display similar correlations in a subset of the objects features. In other words a Correlation Cluster is a group of points which have a low distance $j$, which we will refer to as \textit{jitter}, to a hyperplane $h$ modeled by a correlation within an interval. We formalize it as following:

A correlation cluster in data set $DS$ is a non-empty subset $C^{Corr} \subseteq DS$ of points $p$ which fullfill the following condition:
\begin{align}
    &\forall p: \text{if } \text{dist}(p,h) < j \text{, then } p \in C^{Corr}
\end{align}

where the function $dist(a,b)$ denotes the shortest distance between two geometric\todor{richtiger begriff dafuer?} objects, $h$ being the hyperplane modeling the localized correlation and $j$ the jitter representing the maximal threshold for fault tolerance.

\todor{eigene definition, consistent with dbscan}

\subsubsection*{Correlation Noise}
Analogous to density-based noise in \autoref{ssec:DBSCANindepth}, every point not belonging to any of the existing clusters $C^{Corr}_1, \dotsc, C^{Corr}_k$ in data set $DS$ are now considered as noise: 
\begin{align}
    noise^{Corr} = \{p \in  DS | \forall i : p \notin C^{Corr}_i\}
\end{align}

\subsubsection*{The CASH-Algorithm}
Given previously defined data representation in data and parameter space, the goal is finding dense grid cells in parameter space. However since the complexity of searching the parameter space with a predefined grid is exponential w.r.t.\ dimension $d$ it also quickly gets too computational expensive for higher dimensional spaces. \gls{cash} tackles this problem by smartly dividing the search space. \citeauthor{CASHachtert2008robust} proposed the following search strategy:
\vspace{5mm}

The parameter space is successively divided by the axes in the static order given by $\delta,\alpha_1,\dotsc,\alpha_{d-1}$. After each split the hypercuboid with more intersections is divided at the next axis of the order. If both hypercuboids have the same amount of intersections, the first hypercuboid is selected (arbitrarily). The other hypercuboid is kept in a queue, which is ordered descendingly by the number of its intersecting curves/points. If regions in the queue have an equal amount of intersections, the smaller region is preferred. These splits and additions to the queue are done until both split hypercuboids have less than a minimum number of intersections $m$ to be considered as dense or a predefined depth $s$ has been reached and the hypercuboids fulfilling $m$ and are dense are considered as valid subspace clusters. These valid $(d-1)$-dimensional subspace clusters are then recursively evaluated by CASH again until the resulting found subspace clusters have a minimum dimensionality of $minDims$. If a subspace cluster is found, its intersecting curves are removed from any other hypercuboid in the queue, which is then updated and sorted according to its new status. If the number of intersections drops below $m$, the hypercuboid is removed from the queue. This procedure is done, until the queue is empty.

\begin{algorithm}
% \algsetup{linenosize=\tiny}
% \scriptsize
\SetAlgoLined
\KwData{parameter space $\mathcal{P}$; queue $Q$ ordered by number of intersections and its volume; iterator $I$ of order of axis $[\delta,\alpha_1,\dotsc,\alpha_{d-1}]$; set of subspace clusters $C$; minimum number of intersections $m$; maximum number of splits $s$}
\KwResult{Set of clusters $C$ representing the subspaces found in $\PS$}
\SetKwFunction{CASH}{CASH}
\SetKwProg{Fn}{Function}{:}{\KwRet $C$}
Q := $\emptyset$\;     
h1 := first hypercuboid of split\;
h2 := second hypercuboid of split\;
h := $\PS$\tcp*{working hypercuboid}
\Fn{\CASH{$h$, $m$, $s$}}{
     \Do{$Q$ is not empty}{
        h1, h2 := h.splitAxis($I$.next())\;
        h := $arg\,max_{cube \in \{h1,h2\}}(cube.intersections)$\;
        Q.add(\{h1,h2\}$\setminus$ h)\;
        splitcounter := 1\;
        \While{$h.intersections > m$ and $splitcounter < s$}{
            h1, h2 := h.splitAxis($I$.next())\;
            splitcounter++\;
            h := $arg\,max_{cube \in \{h1,h2\}}(cube.intersections)$\;
            Q.add(\{h1,h2\}$\setminus$ h)\;
        }
        \If{$h.intersections > m$}{
            $C := C\ \cup$\ \CASH{h,m,s}\;
        }
        $I$.reset()\tcp*{reset Iterator for split of next $h$}
        $Q$.removeIntersectionsAndUpdate($C$)\;
    }
}

 \caption{CASH}
\end{algorithm}

The resulting set $C$ contains all $n$-dimensional subspace clusters with $n < d$ found within the constraints of minimum points in correlation $m$ and maximum splits $s$.
\todor{Check recursive CASH}

Summarized we now possess means to find density-based clusters and global subspace clusters in an original data set $DS$. The density-based approach \gls{optics} requires two parameters, minimum number of points $minPts$ and a maximum epsilon distance $\epsilon$, which essentially specify the minimum density allowed, to acquire a hierarchical view of density-connected clusters in $DS$. The subspace clustering approach \gls{cash} also requires another two parameters, minimum number of points/intersections $m$ and number of splits $s$ to be considered as a subspace cluster.

\section{The Algorithm}
With the previously mentioned algorithms we are already able to find arbitrarily shaped clusters with a density-based approach, and arbitrarily oriented subspace clusters with e.g. \gls{4c} or \gls{cash}, however all of them are restricted to finding those clusters in a purely local or purely global fashion. E.g. none of the previously mentioned algorithms can handle scenarios as shown in 
% \begin{minipage}{.47\textwidth}
%       \centering
%       \includegraphics[width=.8\textwidth]{figures/DSwithDBSCANbadBoundingBoxes.pdf}
%       \captionof{figure}{A figure}
%       \label{fig:baddbscan}
%     \end{minipage}%
%     \begin{minipage}{.53 \textwidth}
%       \centering
%       \includegraphics[width=.8\textwidth]{figures/DSwithOPTICSBoundingBoxes.pdf}
%       \captionof{figure}{Another figure}
%       \label{fig:goodoptics}
%     \end{minipage}
\begin{figure}
    \centering
    \includegraphics{}
    \missingfigure[]{scenario zeigen}
    \caption{Caption}
    \label{fig:}
\end{figure}
Above Figures \todor{figure erstellen} depict settings, where the data contains multiple characteristic local correlations which however cannot be accurately described by the aforementioned methods alone. Figure 1 shows cross-shaped dense clusters which can be picked up by \gls{dbscan} or \gls{optics}, however it does not deliver their correlations. As an extension \gls{4c} is able to detect the local correlations, however gets skewed results in the crosses due to the correlations orientation not being accurately representable by the eigenvectors of the \gls{pca}\cite{PCAshlens2014tutorial}. 
\gls{cash} on the other hand could find the correlations. 

\begin{figure}
    \centering
    \includegraphics{}
    \missingfigure[]{local global problem}
    \caption{Caption}
    \label{fig:localglobalproblem}
\end{figure}

\autoref{fig:localglobalproblem} \todor{figure} shows multiple local settings of different correlations. \gls{4c} is able to find the accurate local correlations, however can not categorize those local correlations as one global correlation\todor{naja an sich schon. mit richtigem parameter setting}. 
On the other hand \gls{cash} can only find the global correlation while omitting the information of the gaps in data. Hierarchical subspace clustering methods like \gls{hico} and \gls{eric} do not address the problem either, since their hierarchy is also based on global lower dimensional subspaces contained in the original space and not spaces composed of same dimensional subintervals of the same space. These subintervals of a space further will be referred as \textit{subclusters} or \textit{clusterpartitions}\todor{2 begriffe oder lieber consistent einen benutzen?}.
So either the subspace clustering method detects local correlations while missing the big picture or they do find global correlations while losing structural information of its particular correlation. This thesis proposes an algorithm that unifies local and global correlation clustering by evaluating \textit{locally dense} regions of interest with a correlation clustering method, e.g. \gls{cash}, and combining those local intermediate correlations into a global correlation clustering to get a universal subspace clustering result. 

%Additionally it does not categorize both horizontal local correlations as one, even though they arguably could be interpreted that way.
% , not the big picture

\subsection{Partitioning Data into Dense Clusters}
The first step we perform is the partitioning of the data space into dense clusters via a density-based clustering approach like \gls{dbscan} or \gls{optics}. 
%However like mentioned in \autoref{ssec:DBSCANindepth} and \autoref{ssec:OPTICSindepth}
Since we want to find \textit{local} linear correlations within \textit{global} linear correlations, we assume that the global linear correlation is composed of a set of its local linear correlations. This also means that points belonging to the same global linear correlation are also part of the composing local subclusters which have a similar inherent linear correlation as the global correlation. These subclusters have a continuous distribution of density compared to the global correlation itself, since else it would have been split as well and counted as separate local subcluster. Vice versa the local components are also disjunct to each other, since else there would be a connection to each other to create a bigger clusterpartition. Although it is not optimal even if there is an overlap of different global linear correlations and the clusterpartition contains multiple of them, it still can retain the information of its composing linear correlations.
We can therefore retrieve these local subclusters by searching the data for density-connected sets of points via a density-based clustering approach like \gls{dbscan} or \gls{optics}. This comes with an additional benefit by automatically removing noise. 

However like mentioned in \autoref{ssec:DBSCANindepth} and \autoref{ssec:OPTICSindepth} \gls{dbscan} is not able to find clusters with different density and mixes multiple densities together into one cluster due to its global parameter setting only defining a minimum density. High variance in data densities can therefore strongly influence the result of \gls{dbscan}, either by making it harder to choose the appropriate parameters for a better clustering result or by having more variety in density in found clusters which would result in less conclusive detection of their linear correlations. 
To receive a more suited clustering we preferred the use of \gls{optics}, which is able to detect clusters with different densities (c.f.~\autoref{ssec:OPTICSindepth}). Since we do not want to prune any information away we can generally use a very high epsilon-distance $\epsilon$ to create a detailed ordering and therefore only rely on $MinPts$ as a parameter. From this ordering we can extract clusters with a new parameter $reachdist_{thres}$ which represents the threshold at which the difference of neighboring reachability-distances of the ordering is considered to be a new cluster with different density. We also assign a bounding box for each found cluster, which represents the local data space for further evaluation performed by \gls{cash}. Up to this point we are merely partitioning the data into dense clusters with the added benefit of filtering out points $noise_{density-based}$ which are considered as noise points by a density-based view. We will reconsider those noise points later, since they still could be parts of the global correlation clustering despite not being dense. Figure x visualizes a data set after step 1 is performed. The relevant part of the data set is clustered by density and its boundaries are highlighted by the bounding boxes.
\missingfigure[]{figure with bounding box}

\subsection{Finding Linear Correlations}
Based on the assumption that local linear correlations contained in a global linear correlation have similar orientations as their parents\todor{parent richtiger begriff?}, we extract all correlations of the dense clusters with a subspace clustering method and in a later step combine them and rebuild the global correlation clustering. The second step therefore is the extraction of all local linear correlations of all dense clusters within its respective bounding box in data space. Since we only consider the points of the locally dense subclusters without/less noise we can use a variety of methods to extract the linear correlations via global subspace clustering algorithms. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/PCAdifficulties.png}
    \caption{Caption\cite{PCAshlens2014tutorial}}
    \label{fig:pcadifficulties}
\end{figure}

To be able to get accurate results, a \gls{pca} would be sufficient if the dense subclusters had either only a single linear correlation or almost orthogonal ones. However the density-based clustering in step 1 cannot guarantee this property since it can detect arbitrary shapes. This means, that there could be constellations of points which would heavily skew the results of \gls{pca} (c.f.~\autoref{fig:pcadifficulties})\cite{PCAshlens2014tutorial}. 

Since \gls{4c} is basically looking for dense clusters first and then applying \gls{pca} it has the same disadvantages as \gls{pca} itself~\cite{4cbohm2004computing}. Our whole subcluster already is defined as dense by the previous step and therefore \gls{4c} would label the whole subcluster as dense too and just considers the whole subcluster as its target. Hence applying \gls{pca} afterwards equals to the application of only \gls{pca} onto the whole subcluster. 

\gls{orclus} is also not suited well. It performs a $k$-means like approach first to partition the subcluster~\cite{orclusaggarwal2000finding}, however since our density-based preprocessing step finds different arbitrary shapes, which implies a different number of correlations, in each clusterpartition choosing a meaningful $k$ is difficult or impossible. \todor{soll der teil in related work? Moechte den als uebergang benutzen um die Verwendung von CASH zu begruenden}




After finding all dense regions of interest, the subclusters are independently evaluated for their local correlations without considering the other points in data space. This is done by applying the \gls{cash} algorithm onto the locally dense clusters. \gls{cash} transforms each point of the cluster into the parameter space and splits it according to its parameters minimum Points $m$ and number of splits $s$. 
Apply CASH onto dense Clusters [2]

\subsection{Stitching}

Assembly of local linear correlations [2]